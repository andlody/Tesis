{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "606e4b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.8.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/andlody/Library/Python/3.10/lib/python/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /Users/andlody/Library/Python/3.10/lib/python/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from accelerate) (2.7.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from accelerate) (0.33.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (2024.9.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/andlody/Library/Python/3.10/lib/python/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22f1a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9009f93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Nombre del modelo\n",
    "model_name = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
    "\n",
    "# Paso 1: Descargar el tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained(\"modelos/beto-tokenizer\")\n",
    "\n",
    "# Paso 2: Descargar el modelo base (sin clasificar aÃºn)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.save_pretrained(\"modelos/beto-classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cf21b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"modelos/beto-tokenizer\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"modelos/beto-classifier\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c8b1f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33ed0e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"Â¿CuÃ¡l es la capital de PerÃº?\",\n",
    "        \"Â¿Por quÃ© crees que el arte moderno es tan popular?\",\n",
    "        \"Â¿QuiÃ©n fue el primer presidente del PerÃº?\",\n",
    "        \"Â¿QuÃ© opinas sobre el cambio climÃ¡tico?\",\n",
    "    ],\n",
    "    \"label\": [0, 1, 0, 1]  # 0 = factual, 1 = opinion\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "dataset = Dataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea391b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 656.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b13420c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/czfvlssj4z163jnpkkzwnvgr0000gn/T/ipykernel_73341/2653628895.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Dividir en entrenamiento y validaciÃ³n\n",
    "dataset = dataset.train_test_split(test_size=0.25)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds)\n",
    "    }\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bc2d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04b6f32d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Accelerator.unwrap_model() got an unexpected keyword argument 'keep_torch_compile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/trainer.py:2206\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2207\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2211\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/trainer.py:2332\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mgradient_checkpointing:\n\u001b[1;32m   2330\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgradient_checkpointing_enable(gradient_checkpointing_kwargs\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_checkpointing_kwargs)\n\u001b[0;32m-> 2332\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_wrapped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2334\u001b[0m \u001b[38;5;66;03m# as the model is wrapped, don't use `accelerator.prepare`\u001b[39;00m\n\u001b[1;32m   2335\u001b[0m \u001b[38;5;66;03m# this is for unhandled cases such as\u001b[39;00m\n\u001b[1;32m   2336\u001b[0m \u001b[38;5;66;03m# FSDP-XLA, SageMaker MP/DP, DataParallel, IPEX\u001b[39;00m\n\u001b[1;32m   2337\u001b[0m use_accelerator_prepare \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/trainer.py:1954\u001b[0m, in \u001b[0;36mTrainer._wrap_model\u001b[0;34m(self, model, training, dataloader)\u001b[0m\n\u001b[1;32m   1951\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m smp\u001b[38;5;241m.\u001b[39mDistributedModel(model, backward_passes_per_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps)\n\u001b[1;32m   1953\u001b[0m \u001b[38;5;66;03m# train/eval could be run multiple-times - if already wrapped, don't re-wrap it again\u001b[39;00m\n\u001b[0;32m-> 1954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munwrap_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_torch_compile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model:\n\u001b[1;32m   1955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;66;03m# Mixed precision training with apex\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Accelerator.unwrap_model() got an unexpected keyword argument 'keep_torch_compile'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe28860",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Â¿QuÃ© sabes del Imperio Inca?\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "outputs = model(**inputs)\n",
    "predicted_class = outputs.logits.argmax().item()\n",
    "\n",
    "print(\"PredicciÃ³n:\", \"Factual\" if predicted_class == 0 else \"OpiniÃ³n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ec8dc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 1638.78 examples/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Accelerator.unwrap_model() got an unexpected keyword argument 'keep_torch_compile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 54\u001b[0m\n\u001b[1;32m     36\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     37\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#num_train_epochs=4,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     eval_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     47\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     48\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m     52\u001b[0m )\n\u001b[0;32m---> 54\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/trainer.py:2206\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2207\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2211\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/trainer.py:2332\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mgradient_checkpointing:\n\u001b[1;32m   2330\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgradient_checkpointing_enable(gradient_checkpointing_kwargs\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_checkpointing_kwargs)\n\u001b[0;32m-> 2332\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_wrapped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2334\u001b[0m \u001b[38;5;66;03m# as the model is wrapped, don't use `accelerator.prepare`\u001b[39;00m\n\u001b[1;32m   2335\u001b[0m \u001b[38;5;66;03m# this is for unhandled cases such as\u001b[39;00m\n\u001b[1;32m   2336\u001b[0m \u001b[38;5;66;03m# FSDP-XLA, SageMaker MP/DP, DataParallel, IPEX\u001b[39;00m\n\u001b[1;32m   2337\u001b[0m use_accelerator_prepare \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/trainer.py:1954\u001b[0m, in \u001b[0;36mTrainer._wrap_model\u001b[0;34m(self, model, training, dataloader)\u001b[0m\n\u001b[1;32m   1951\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m smp\u001b[38;5;241m.\u001b[39mDistributedModel(model, backward_passes_per_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps)\n\u001b[1;32m   1953\u001b[0m \u001b[38;5;66;03m# train/eval could be run multiple-times - if already wrapped, don't re-wrap it again\u001b[39;00m\n\u001b[0;32m-> 1954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munwrap_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_torch_compile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model:\n\u001b[1;32m   1955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;66;03m# Mixed precision training with apex\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Accelerator.unwrap_model() got an unexpected keyword argument 'keep_torch_compile'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# âœ… Datos de entrenamiento\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"Â¿Tienen disponible la lavadora Samsung?\",\n",
    "        \"Â¿Hay stock de sillas de oficina?\",\n",
    "        \"Â¿EstÃ¡ en promociÃ³n la cocina empotrable?\",\n",
    "        \"Â¿CuÃ¡nto cuesta este taladro?\",\n",
    "        \"Â¿Puedo recogerlo en tienda?\",\n",
    "        \"Â¿QuÃ© tal es la calidad de este escritorio?\",\n",
    "        \"Â¿CuÃ¡l es la mejor marca para taladros?\",\n",
    "        \"Â¿QuÃ© me recomiendan para una sala pequeÃ±a?\",\n",
    "        \"Â¿CuÃ¡l es la diferencia entre estos dos modelos?\",\n",
    "        \"Â¿QuÃ© opciones hay para baÃ±os pequeÃ±os?\"\n",
    "    ],\n",
    "    \"label\": [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]  # 0 = Cerrada, 1 = Abierta\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# âœ… Cargar modelo y tokenizer BETO\n",
    "model_name = \"dccuchile/bert-base-spanish-wwm-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# âœ… Tokenizar el dataset\n",
    "def preprocess(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess)\n",
    "\n",
    "# âœ… Entrenamiento rÃ¡pido\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    #num_train_epochs=4,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_steps=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    eval_strategy=\"no\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fc6a802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Define the model name\n",
    "model_name = \"dccuchile/bert-base-spanish-wwm-uncased\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"Este es un ejemplo en espaÃ±ol.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Print the hidden states (example)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5ae0b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3476, -0.1922, -0.1956,  ..., -0.4622,  0.1942,  0.0063],\n",
      "         [ 0.2579, -0.4332, -0.1984,  ..., -0.4033, -0.1360,  0.0243],\n",
      "         [-0.7633,  0.0132, -0.0737,  ..., -0.0311, -0.5246, -0.0170],\n",
      "         ...,\n",
      "         [ 0.0064, -0.5103, -0.3769,  ..., -0.6194,  0.3948, -0.6656],\n",
      "         [-0.3284, -0.1754, -0.2113,  ..., -0.0937,  0.2410, -0.1889],\n",
      "         [-0.8958, -1.0060, -0.5904,  ..., -0.5459, -0.1394, -0.8056]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')\n",
    "model = BertModel.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')\n",
    "\n",
    "# Pre-process input text\n",
    "input_text = \"Hola, Â¿cÃ³mo estÃ¡s?\"\n",
    "inputs = tokenizer.encode_plus(input_text, \n",
    "                                add_special_tokens=True, \n",
    "                                max_length=512, \n",
    "                                return_attention_mask=True, \n",
    "                                return_tensors='pt')\n",
    "\n",
    "# Run input through model\n",
    "outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "\n",
    "# Print output\n",
    "print(outputs.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0d276aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.38.2\n",
      "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "Collecting accelerate==0.27.2\n",
      "  Downloading accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.38.2) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.38.2) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.38.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/andlody/Library/Python/3.10/lib/python/site-packages (from transformers==4.38.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.38.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.38.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.38.2) (2.32.3)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.2)\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.38.2) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers==4.38.2) (4.67.1)\n",
      "Requirement already satisfied: psutil in /Users/andlody/Library/Python/3.10/lib/python/site-packages (from accelerate==0.27.2) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from accelerate==0.27.2) (2.7.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/andlody/Library/Python/3.10/lib/python/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (1.1.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.10.0->accelerate==0.27.2) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.27.2) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers==4.38.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers==4.38.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers==4.38.2) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers==4.38.2) (2025.4.26)\n",
      "Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
      "Downloading tokenizers-0.15.2-cp310-cp310-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, accelerate, transformers\n",
      "\u001b[2K  Attempting uninstall: tokenizers\n",
      "\u001b[2K    Found existing installation: tokenizers 0.21.2\n",
      "\u001b[2K    Uninstalling tokenizers-0.21.2:\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.21.2\n",
      "\u001b[2K  Attempting uninstall: accelerate\n",
      "\u001b[2K    Found existing installation: accelerate 1.8.1\n",
      "\u001b[2K    Uninstalling accelerate-1.8.1:\n",
      "\u001b[2K      Successfully uninstalled accelerate-1.8.1\n",
      "\u001b[2K  Attempting uninstall: transformers\n",
      "\u001b[2K    Found existing installation: transformers 4.53.2\n",
      "\u001b[2K    Uninstalling transformers-4.53.2:m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2/3\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.53.290mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2/3\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/3\u001b[0m [transformers][0m [transformers]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autotrain-advanced 0.8.36 requires accelerate==1.2.1, but you have accelerate 0.27.2 which is incompatible.\n",
      "autotrain-advanced 0.8.36 requires huggingface-hub==0.27.0, but you have huggingface-hub 0.33.4 which is incompatible.\n",
      "autotrain-advanced 0.8.36 requires packaging==24.2, but you have packaging 25.0 which is incompatible.\n",
      "autotrain-advanced 0.8.36 requires Pillow==11.0.0, but you have pillow 11.3.0 which is incompatible.\n",
      "autotrain-advanced 0.8.36 requires transformers==4.48.0, but you have transformers 4.38.2 which is incompatible.\n",
      "mlx-lm 0.26.0 requires transformers>=4.39.3, but you have transformers 4.38.2 which is incompatible.\n",
      "sentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.2 which is incompatible.\n",
      "trl 0.13.0 requires accelerate>=0.34.0, but you have accelerate 0.27.2 which is incompatible.\n",
      "trl 0.13.0 requires transformers>=4.46.0, but you have transformers 4.38.2 which is incompatible.\n",
      "vllm 0.9.2 requires tokenizers>=0.21.1, but you have tokenizers 0.15.2 which is incompatible.\n",
      "vllm 0.9.2 requires torch==2.7.0; platform_system == \"Darwin\", but you have torch 2.7.1 which is incompatible.\n",
      "vllm 0.9.2 requires transformers>=4.51.1, but you have transformers 4.38.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.27.2 tokenizers-0.15.2 transformers-4.38.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers==4.38.2 accelerate==0.27.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a318e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.38.2\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: autotrain-advanced, compressed-tensors, mlx-lm, peft, sentence-transformers, trl, vllm, xgrammar\n",
      "---\n",
      "Name: accelerate\n",
      "Version: 0.27.2\n",
      "Summary: Accelerate\n",
      "Home-page: https://github.com/huggingface/accelerate\n",
      "Author: The HuggingFace team\n",
      "Author-email: sylvain@huggingface.co\n",
      "License: Apache\n",
      "Location: /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages\n",
      "Requires: huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch\n",
      "Required-by: autotrain-advanced, peft, trl\n",
      "---\n",
      "Name: torch\n",
      "Version: 2.7.1\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3-Clause\n",
      "Location: /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: accelerate, compressed-tensors, facenet-pytorch, outlines, peft, sentence-transformers, timm, torchaudio, torchmetrics, torchvision, vllm, xgrammar\n"
     ]
    }
   ],
   "source": [
    "!pip3 show transformers accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82706854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 855.06 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 675.52 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/training_args.py:1604: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Accelerator.unwrap_model() got an unexpected keyword argument 'keep_torch_compile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 72\u001b[0m\n\u001b[1;32m     63\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     64\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     65\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics    \n\u001b[1;32m     69\u001b[0m )\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Entrenamiento\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# EvaluaciÃ³n\u001b[39;00m\n\u001b[1;32m     75\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/trainer.py:2206\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2207\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2211\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/trainer.py:2332\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mgradient_checkpointing:\n\u001b[1;32m   2330\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgradient_checkpointing_enable(gradient_checkpointing_kwargs\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_checkpointing_kwargs)\n\u001b[0;32m-> 2332\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_wrapped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2334\u001b[0m \u001b[38;5;66;03m# as the model is wrapped, don't use `accelerator.prepare`\u001b[39;00m\n\u001b[1;32m   2335\u001b[0m \u001b[38;5;66;03m# this is for unhandled cases such as\u001b[39;00m\n\u001b[1;32m   2336\u001b[0m \u001b[38;5;66;03m# FSDP-XLA, SageMaker MP/DP, DataParallel, IPEX\u001b[39;00m\n\u001b[1;32m   2337\u001b[0m use_accelerator_prepare \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/trainer.py:1954\u001b[0m, in \u001b[0;36mTrainer._wrap_model\u001b[0;34m(self, model, training, dataloader)\u001b[0m\n\u001b[1;32m   1951\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m smp\u001b[38;5;241m.\u001b[39mDistributedModel(model, backward_passes_per_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps)\n\u001b[1;32m   1953\u001b[0m \u001b[38;5;66;03m# train/eval could be run multiple-times - if already wrapped, don't re-wrap it again\u001b[39;00m\n\u001b[0;32m-> 1954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munwrap_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_torch_compile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model:\n\u001b[1;32m   1955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;66;03m# Mixed precision training with apex\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Accelerator.unwrap_model() got an unexpected keyword argument 'keep_torch_compile'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Detectar si hay GPU (opcional, pero usamos CPU aquÃ­)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Dataset de ejemplo (puedes ampliarlo con mÃ¡s datos reales)\n",
    "data = {\n",
    "    'text': [\n",
    "        \"Â¿CuÃ¡l es tu comida favorita?\",          # Abierta\n",
    "        \"Â¿Te gusta el chocolate?\",               # Cerrada\n",
    "        \"Â¿Por quÃ© elegiste esa carrera?\",        # Abierta\n",
    "        \"Â¿Vienes maÃ±ana?\",                       # Cerrada\n",
    "        \"Â¿CÃ³mo te sentiste despuÃ©s del examen?\", # Abierta\n",
    "        \"Â¿Tienes hambre?\"                        # Cerrada\n",
    "    ],\n",
    "    'label': [1, 0, 1, 0, 1, 0]  # 1 = abierta, 0 = cerrada\n",
    "}\n",
    "\n",
    "# Crear dataset\n",
    "dataset = Dataset.from_dict(data)\n",
    "train_test = dataset.train_test_split(test_size=0.3)\n",
    "train_dataset = train_test['train']\n",
    "test_dataset = train_test['test']\n",
    "\n",
    "# Tokenizador BETO\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(example['text'], padding='max_length', truncation=True, max_length=64)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize)\n",
    "test_dataset = test_dataset.map(tokenize)\n",
    "\n",
    "# Modelo BETO para clasificaciÃ³n binaria\n",
    "model = BertForSequenceClassification.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\", num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# Funciones de mÃ©trica\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    acc = accuracy_score(p.label_ids, preds)\n",
    "    f1 = f1_score(p.label_ids, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# Argumentos de entrenamiento (usando CPU)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./beto-preguntas\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    no_cuda=True  # fuerza uso de CPU\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics    \n",
    ")\n",
    "\n",
    "# Entrenamiento\n",
    "trainer.train()\n",
    "\n",
    "# EvaluaciÃ³n\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "61a48fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.8.7)\n",
      "Collecting spacy-lookups-data\n",
      "  Downloading spacy_lookups_data-1.0.5-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (2.10.4)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (79.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/andlody/Library/Python/3.10/lib/python/site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/andlody/Library/Python/3.10/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.2.6-cp310-cp310-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/andlody/Library/Python/3.10/lib/python/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Using cached numpy-2.2.6-cp310-cp310-macosx_14_0_arm64.whl (5.3 MB)\n",
      "Downloading spacy_lookups_data-1.0.5-py2.py3-none-any.whl (98.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m98.5/98.5 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: spacy-lookups-data, numpy\n",
      "\u001b[2K  Attempting uninstall: numpyâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0/2\u001b[0m [spacy-lookups-data]\n",
      "\u001b[2K    Found existing installation: numpy 1.26.4[0m \u001b[32m0/2\u001b[0m [spacy-lookups-data]\n",
      "\u001b[2K    Uninstalling numpy-1.26.4:â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0/2\u001b[0m [spacy-lookups-data]\n",
      "\u001b[2K      Successfully uninstalled numpy-1.26.4â”\u001b[0m \u001b[32m0/2\u001b[0m [spacy-lookups-data]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2/2\u001b[0m [numpy]32m1/2\u001b[0m [numpy]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autotrain-advanced 0.8.36 requires accelerate==1.2.1, but you have accelerate 0.27.2 which is incompatible.\n",
      "autotrain-advanced 0.8.36 requires huggingface-hub==0.27.0, but you have huggingface-hub 0.33.4 which is incompatible.\n",
      "autotrain-advanced 0.8.36 requires packaging==24.2, but you have packaging 25.0 which is incompatible.\n",
      "autotrain-advanced 0.8.36 requires Pillow==11.0.0, but you have pillow 11.3.0 which is incompatible.\n",
      "autotrain-advanced 0.8.36 requires transformers==4.48.0, but you have transformers 4.38.2 which is incompatible.\n",
      "facenet-pytorch 2.6.0 requires numpy<2.0.0,>=1.24.0, but you have numpy 2.2.6 which is incompatible.\n",
      "facenet-pytorch 2.6.0 requires Pillow<10.3.0,>=10.2.0, but you have pillow 11.3.0 which is incompatible.\n",
      "facenet-pytorch 2.6.0 requires torch<2.3.0,>=2.2.0, but you have torch 2.7.1 which is incompatible.\n",
      "facenet-pytorch 2.6.0 requires torchvision<0.18.0,>=0.17.0, but you have torchvision 0.22.0 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\n",
      "mediapipe 0.10.21 requires numpy<2, but you have numpy 2.2.6 which is incompatible.\n",
      "mlx-lm 0.26.0 requires transformers>=4.39.3, but you have transformers 4.38.2 which is incompatible.\n",
      "sentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.2 which is incompatible.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
      "tensorflow 2.19.0 requires tensorboard~=2.19.0, but you have tensorboard 2.18.0 which is incompatible.\n",
      "torchvision 0.22.0 requires torch==2.7.0, but you have torch 2.7.1 which is incompatible.\n",
      "trl 0.13.0 requires accelerate>=0.34.0, but you have accelerate 0.27.2 which is incompatible.\n",
      "trl 0.13.0 requires transformers>=4.46.0, but you have transformers 4.38.2 which is incompatible.\n",
      "vllm 0.9.2 requires tokenizers>=0.21.1, but you have tokenizers 0.15.2 which is incompatible.\n",
      "vllm 0.9.2 requires torch==2.7.0; platform_system == \"Darwin\", but you have torch 2.7.1 which is incompatible.\n",
      "vllm 0.9.2 requires transformers>=4.51.1, but you have transformers 4.38.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.6 spacy-lookups-data-1.0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U spacy spacy-lookups-data\n",
    "!python3 -m spacy download es_core_news_sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69329bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_data = [\n",
    "    (\"Â¿Te gusta el helado?\", {\"cats\": {\"cerrada\": 1.0, \"abierta\": 0.0}}),\n",
    "    (\"Â¿Por quÃ© el cielo es azul?\", {\"cats\": {\"cerrada\": 0.0, \"abierta\": 1.0}}),\n",
    "    (\"Â¿QuÃ© piensas de la polÃ­tica actual?\", {\"cats\": {\"cerrada\": 0.0, \"abierta\": 1.0}}),\n",
    "    (\"Â¿Vas a venir maÃ±ana?\", {\"cats\": {\"cerrada\": 1.0, \"abierta\": 0.0}}),\n",
    "    # Agrega mÃ¡s ejemplos...\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bed42771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training.example import Example\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\", disable=[\"lemmatizer\"])\n",
    "if \"textcat\" not in nlp.pipe_names:\n",
    "    textcat = nlp.add_pipe(\"textcat\", last=True)\n",
    "else:\n",
    "    textcat = nlp.get_pipe(\"textcat\")\n",
    "\n",
    "# â€¦ resto del cÃ³digo exactamente igual â€¦\n",
    "\n",
    "\n",
    "textcat.add_label(\"abierta\")\n",
    "textcat.add_label(\"cerrada\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "58d024be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IteraciÃ³n 0 - PÃ©rdidas: {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 0.0, 'textcat': 0.25}\n",
      "IteraciÃ³n 1 - PÃ©rdidas: {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 0.0, 'textcat': 0.23490776121616364}\n",
      "IteraciÃ³n 2 - PÃ©rdidas: {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 0.0, 'textcat': 0.1970246285200119}\n",
      "IteraciÃ³n 3 - PÃ©rdidas: {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 0.0, 'textcat': 0.15198659896850586}\n",
      "IteraciÃ³n 4 - PÃ©rdidas: {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 0.0, 'textcat': 0.1063109040260315}\n",
      "IteraciÃ³n 5 - PÃ©rdidas: {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 0.0, 'textcat': 0.06790086627006531}\n",
      "IteraciÃ³n 6 - PÃ©rdidas: {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 0.0, 'textcat': 0.03979098051786423}\n",
      "IteraciÃ³n 7 - PÃ©rdidas: {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 0.0, 'textcat': 0.021397341042757034}\n",
      "IteraciÃ³n 8 - PÃ©rdidas: {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 0.0, 'textcat': 0.010769391432404518}\n",
      "IteraciÃ³n 9 - PÃ©rdidas: {'tok2vec': 0.0, 'morphologizer': 0.0, 'parser': 0.0, 'ner': 0.0, 'textcat': 0.005062449257820845}\n"
     ]
    }
   ],
   "source": [
    "from spacy.training.example import Example\n",
    "\n",
    "optimizer = nlp.initialize()\n",
    "n_iter = 10\n",
    "\n",
    "for i in range(n_iter):\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "    batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        examples = []\n",
    "        for text, annotations in batch:\n",
    "            doc = nlp.make_doc(text)\n",
    "            examples.append(Example.from_dict(doc, annotations))\n",
    "        nlp.update(examples, sgd=optimizer, losses=losses)\n",
    "    print(f\"IteraciÃ³n {i} - PÃ©rdidas:\", losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6665bc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClasificaciÃ³n: cerrada, puntajes: {'abierta': 0.45497769117355347, 'cerrada': 0.5450223088264465}\n"
     ]
    }
   ],
   "source": [
    "def clasificar_pregunta(pregunta):\n",
    "    doc = nlp(pregunta)\n",
    "    scores = doc.cats\n",
    "    etiqueta = max(scores, key=scores.get)\n",
    "    return etiqueta, scores\n",
    "\n",
    "#ejemplo = \"Â¿CuÃ¡l es tu comida favorita?\"\n",
    "ejemplo = \"Â¿Quieres que te prepare una comida?\"\n",
    "etiqueta, scores = clasificar_pregunta(ejemplo)\n",
    "print(f\"ClasificaciÃ³n: {etiqueta}, puntajes: {scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266f4133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71aa2d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, jsonify\n",
    "app = Flask(__name__)\n",
    "@app.route('/saludo', methods=['GET'])\n",
    "def saludo():\n",
    "    return jsonify({'mensaje': 'Hola desde Colab!'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37e41b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyngrok in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (7.2.1)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pyngrok) (6.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48e569b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [24172]\n",
      "INFO:     Waiting for application startup.\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1' coro=<Server.serve() done, defined at /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/uvicorn/main.py\", line 579, in run\n",
      "    server.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/uvicorn/server.py\", line 66, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "  File \"/Users/andlody/Library/Python/3.10/lib/python/site-packages/nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "  File \"/Users/andlody/Library/Python/3.10/lib/python/site-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"/Users/andlody/Library/Python/3.10/lib/python/site-packages/nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py\", line 315, in __wakeup\n",
      "    self.__step()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/uvicorn/server.py\", line 69, in serve\n",
      "    with self.capture_signals():\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/contextlib.py\", line 142, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/uvicorn/server.py\", line 330, in capture_signals\n",
      "    signal.raise_signal(captured_signal)\n",
      "KeyboardInterrupt\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10' coro=<Server.serve() done, defined at /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/uvicorn/main.py\", line 579, in run\n",
      "    server.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/uvicorn/server.py\", line 66, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "  File \"/Users/andlody/Library/Python/3.10/lib/python/site-packages/nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "  File \"/Users/andlody/Library/Python/3.10/lib/python/site-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"/Users/andlody/Library/Python/3.10/lib/python/site-packages/nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py\", line 315, in __wakeup\n",
      "    self.__step()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/uvicorn/server.py\", line 69, in serve\n",
      "    with self.capture_signals():\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/contextlib.py\", line 142, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/uvicorn/server.py\", line 330, in capture_signals\n",
      "    signal.raise_signal(captured_signal)\n",
      "KeyboardInterrupt\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:61305 - \"GET /clasificar HTTP/1.1\" 422 Unprocessable Entity\n",
      "INFO:     127.0.0.1:61312 - \"GET /clasificar HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:61319 - \"GET /clasificar HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:61328 - \"GET /clasificar HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [24172]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from pyngrok import ngrok\n",
    "ngrok.set_auth_token(\"1o4ItumAvMRlaYTG9dxyHKNInZq_48f9Txagc76SkaUUpoZB4\")\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_stdlib_context\n",
    "\n",
    "public_url = ngrok.connect(8080)\n",
    "print(public_url)\n",
    "'''\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "#from pyngrok import ngrok\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "\n",
    "# Necesario para que funcione en Colab\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Define la app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define un esquema de entrada\n",
    "class Pregunta(BaseModel):\n",
    "    texto: str\n",
    "    lol: list\n",
    "\n",
    "# Endpoint de prueba\n",
    "@app.get(\"/clasificar\")\n",
    "def clasificar(pregunta: Pregunta):\n",
    "    texto = pregunta.texto\n",
    "    lol = pregunta.lol\n",
    "    # AquÃ­ puedes poner el matcher de spaCy o un clasificador\n",
    "    return {\"categoria\": \"CONSULTA_RECOMENDACION\",\"lolo\":lol}\n",
    "\n",
    "# Inicia el tunel ngrok\n",
    "#public_url = ngrok.connect(8000)\n",
    "#print(f\"ðŸ”— API disponible en: {public_url}\")\n",
    "\n",
    "# Inicia el servidor\n",
    "uvicorn.run(app, port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad6429db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bert-score) (2.7.0)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bert-score) (2.2.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bert-score) (4.53.2)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bert-score) (2.1.3)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bert-score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bert-score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bert-score) (3.10.3)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/andlody/Library/Python/3.10/lib/python/site-packages (from bert-score) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/andlody/Library/Python/3.10/lib/python/site-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/andlody/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/andlody/Library/Python/3.10/lib/python/site-packages (from torch>=1.0.0->bert-score) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (2024.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.33.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.5.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->bert-score) (1.1.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->bert-score) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->bert-score) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->bert-score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->bert-score) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->bert-score) (3.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->bert-score) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->bert-score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->bert-score) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->bert-score) (2025.4.26)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: bert-score\n",
      "Successfully installed bert-score-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip3 install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35d1779b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 490.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.61 seconds, 8.22 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bert_score import score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Candidatos y referencias\n",
    "candidatos = [\n",
    "    \"El gato estÃ¡ sentado sobre la alfombra\",\n",
    "    \"El perro ladra fuerte\",\n",
    "    \"El ovalin es un prodcto bueno\",\n",
    "    \"El perro ladra fuerte\",\n",
    "    \"El perro ladra fuerte\"\n",
    "]\n",
    "\n",
    "referencias = [\n",
    "    \"El felino se encuentra en la alfombra\",\n",
    "    \"El canino hace ruido\",\n",
    "    \"El ovalin es un prodcto malo\",\n",
    "    \"El perro ladra fuerte\",\n",
    "    \"El perro ladra fuerte\"\n",
    "]\n",
    "\n",
    "# Calcular BERTScore\n",
    "P, R, F1 = score(candidatos, referencias, lang=\"es\", verbose=True)\n",
    "\n",
    "# Convertir a listas de floats\n",
    "P = P.tolist()\n",
    "R = R.tolist()\n",
    "F1 = F1.tolist()\n",
    "\n",
    "# Crear grÃ¡fico de barras para cada mÃ©trica por ejemplo\n",
    "x = range(len(candidatos))  # Ã­ndices de cada par de ejemplo\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x, P, width=0.25, label=\"PrecisiÃ³n\", align='center')\n",
    "plt.bar([i + 0.25 for i in x], R, width=0.25, label=\"Recall\", align='center')\n",
    "plt.bar([i + 0.50 for i in x], F1, width=0.25, label=\"F1\", align='center')\n",
    "\n",
    "# Etiquetas y tÃ­tulo\n",
    "plt.xlabel(\"Ejemplos\")\n",
    "plt.ylabel(\"PuntuaciÃ³n\")\n",
    "plt.title(\"BERTScore por ejemplo\")\n",
    "plt.xticks([i + 0.25 for i in x], [f\"Ej {i+1}\" for i in x])\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
